\documentclass[article]{jss}
\usepackage{algorithmic}
\usepackage{algorithm} 


%%\VignetteIndexEntry{Causal Inference: The pcalg R package}
%%\VignetteDepends{}
\SweaveOpts{engine=R,eps=FALSE,pdf=TRUE,width=7,height=4}
\SweaveOpts{keep.source=TRUE,strip.white=TRUE}
%           ^^^^^^^^^^^^^^^^ preserve comments (and all) in R chunks

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{Markus Kalisch\\ETH Z\"urich \And
        Martin M\"achler\\ETH Z\"urich \And
      Diego Colombo\\ETH Z\"urich \And
    Marloes H. Maathuis\\ETH Z\"urich \And
  Peter B\"uhlmann\\ETH Z\"urich}
\title{Causal Inference using Graphical Models with the Package \pkg{pcalg}}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Markus Kalisch, Martin M\"achler, Diego Colombo, Marloes
  H. Maathuis, Peter B\"uhlmann} %% comma-separated
\Plaintitle{Causal Inference using Graphical Models: The package pcalg} %% without formatting
\Shorttitle{Causal Inference} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{ The \pkg{pcalg} package for \proglang{R} is a tool for
  estimating intervention effects when the true underlying causal structure
  is unknown. To this end, \pkg{pcalg} contains functions for estimating
  the causal structure using graphical models (functions \code{skeleton},
  \code{pc} and \code{fci}) and functions for estimating intervention
  effects given an estimate of the causal structure (functions \code{ida}
  and \code{idaFast}). In this document, we will give a brief overview of the
  methodological background of estimating intervention effects. Moreover,
  we will demonstrate how the package \pkg{pcalg} can be used for
  estimating intervention effects and graphical models using examples.} 

\Keywords{IDA, PC, FCI, do-calculus, causality, graphical models, \proglang{R}} 
\Plainkeywords{IDA, PC, FCI, do-calculus, causality, graphical models, R} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{13}
%% \Issue{9}
%% \Month{September}
%% \Year{2004}
%% \Submitdate{2004-09-29}
%% \Acceptdate{2004-09-29}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Markus Kalisch\\
  Seminar f\"ur Statistik\\
  ETH Z\"urich\\
  8092 Z\"urich, Switzerland\\
  E-mail: \email{kalisch@stat.math.ethz.ch}\\
  %% URL: \url{http://stat.ethz.ch/people/kalisch}
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/1/31336-5053
%% Fax: +43/1/31336-734

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}


%% include your article here, just as usual
%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

\section{Introduction}
THIS DOCUMENTATION IS STILL UNDER CONSTRUCTION! \\

Understanding cause-effect relationships between variables is of primary
interest in many fields of science. Usually, experimental intervention is
used to find these relationships. In many settings, however, experiments
are infeasible because of time, cost or ethical constraints.

Recently, we proposed and mathematically justified a statistical method
(IDA) to obtain bounds on total causal effects based solely on
observational data under some assumptions (see \cite{BuKaMa10}). Furthermore, we presented an
experimental validation of our method on a large-scale biological system
(see \cite{NatMethods10}).

For further validation and broader use of this method, well documented and
easy to use software is indispensable. Therefore, we wrote the R package
\pkg{pcalg}, which incorporates the above mentioned method (IDA).

The objective of this paper is to introduce the R package \pkg{pcalg} and
explain the range of functions.

To get started, we show how two of the main functions can be used in a
typical application. Suppose we have a system described by some
variables and many observations of this system. Furthermore, assume
that it seems plausible that there are no hidden variables and no
feedback loops in the underlying causal system. The causal structure
of such a system can be conveniently represented by a Directed Acyclic
Graph (DAG), where each node represents a variable and each arrow
represents a direct cause. We are interested in the change of variable
Y if we changed the variable X by intervention, i.e., we seek the
causal effect of X on Y.  To fix ideas, we have simulated an example
data set with $p = 8$ continuous variables with Gaussian noise and $n
= 5000$ observations, which we will now analyse. First, we load the
package \pkg{pcalg} and the data set.

<<exIntro1, echo = TRUE>>=
 library(pcalg, lib.loc = "/sfs/u/staff/kalisch/research/packages/pcalg.Rcheck")
 data(gaussianData)
 dim(datG)
@

In the next step, we use the function \code{pc} to produce an estimate
of the underlying causal structure. Since this function is based on
conditional independence tests, we need to define two things. First,
we need a function that can compute conditional independence tests in
a way that is suitable for the data at hand. For standard data types
(Gaussian, discrete and binary) we provide predefined functions. See
the example section in the help file of \code{pc} for more
details. Secondly, we need a summary of the data (sufficient
statistic) on which the conditional independence function can
work. Each conditional independence test can be performed at a certain
significance level \code{alpha}. This can be treated as a tuning
parameter. In the following code, we use the predefined function
\code{gaussCItest} as conditional independence test and create the
corresponding sufficient statistic. The sufficient statistic consists
of the correlation matrix of the data and the sample size. Then we use
the function \code{pc} to estimate the causal structure and plot the
result.

<<exIntroPlot>>=
indepTest <- gaussCItest 
suffStat <- list(C = cor(datG), n = 5000)
pc.fit <- pc(suffStat, indepTest, p = 8, alpha = 0.01)
require(Rgraphviz)
par(mfrow = c(1,2))
plot(gGtrue, main = "True DAG")
plot(pc.fit, main = "Estimated DAG")
@ 

\begin{figure}
  \begin{center}
<<exIntroPlot, echo = FALSE, fig = TRUE>>=
require(Rgraphviz)
par(mfrow = c(1,2))
plot(gGtrue, main = "True DAG")
plot(pc.fit, main = "Estimated DAG")
@ 
\caption{True and estimated causal structure represented by a Directed
  Acyclic Graph (DAG).}
\label{fig:intro1}
\end{center}
\end{figure}

As can be seen in Fig. \ref{fig:intro1}, there are directed and bidirected
edges in the estimated causal structure. The directed edges show the
presence and direction of direct causal effects. The direction of the
bidirected edges, however, could not be decided by our method. Thus, they
represent some uncertainty in the resulting model. A fundamental property
of our method is, that some uncertainty of this kind sometimes remains,
even if an infinite amount of data is available.

Based on the inferred causal structure, we can estimate the causal effect
of an intervention. For example, suppose, we would increase variable $V1$
by external intervention by one unit. The recorded increase in variable
$V6$ is the (total) causal effect of $V1$ on $V6$. More precisely, the
causal effect $C(V1, V6, \tilde{x})$ of $V1$ from $V1 = \tilde{x}$ on $V6$
is defined as
\begin{eqnarray*}
C(V1, V6, \tilde{x}) &=& E(V1|do(V6 = \tilde{x} + 1)) - E(V1|do(V6 = \tilde{x})) \ \mbox{or} \\
C(V1, V6, \tilde{x}) &=& \frac{\partial}{\partial x} E(V1|do(V6 = x))|_{x=\tilde{x}}
\end{eqnarray*}
If the causal relationships are linear, the causal effect is independent of
$\tilde{x}$.  

Since the causal structure was not identified perfectly in our example, we
cannot expect to get a unique number for the causal effect. Instead, we will get a set of
possible causal effects. This set can be computed by using the function
\code{ida}. To provide full quantitative information, we need to pass the
covariance matrix in addition to the estimated causal structure.

<<exIntro2>>=
ida(1, 6, cov(datG), pc.fit@graph)
@ 

Since we simulated the data, we know that the true value of the causal
effect is 0.296. Thus, one of the two estimates is indeed close to the true
value. Since both values are larger than zero, we can conclude, that
variable $V1$ has a positive causal effect on variable $V6$. Thus, we can
always estimate a lower bound to the absolute value of the causal
effect. (Note that at this point we have no p-value to control the sampling
error).

If we would like to know the effect of a unit increase in variable $V1$ on
variables $V3$, $V5$ and $V6$, we could simply call \code{ida} three
times. However, a faster way is to call the function \code{idaFast}, which
was tailored for such situations. 

<<exIntro3>>=
idaFast(1, c(3,5,6), cov(datG), pc.fit@graph)
@ 

Each row in the output shows the estimated set of possible causal
effects on the target variable indicated by the row names.  The true
values for the causal effects are 0.441, 0 and 0.296 for variables $V3$,
$V5$ and $V6$, respectively. The first row of the output,
corresponding to variable $V3$, is rather uninformative: Although one entry
comes close to the true value, the other estimate is 0. Thus, we
cannot be sure if there is a causal effect at all. The second row,
corresponding to variable $V5$, quite accurately indicates a causal
effect that is very close to zero or no effect at all. The third row,
corresponding to variable $V6$, was already discussed above.

\section{Methodological background}
Our proposed method consists of two major steps. In the first step, the
causal structure is estimated. This is done by estimating a graphical
model. A graphical model is a map of the dependence structure of the data
and can thus be an interesting object by itself. In the second step, we use
the estimated causal structure and the do-calculus (see \cite{Pearl00}) to calculate
bounds on causal effects.

\subsection{Estimating graphical models} \label{sec:gm} 
Graphical models
can be thought of as maps of dependence structures of a given probability
distribution or a sample thereof (see for example \cite{lauritzen}). In
order to illustrate the analogy, let us consider a road map. In order to be
able to use a road map, one needs two given factors. Firstly, one needs the
physical map with symbols such as dots and lines. Secondly, one needs a
rule for interpreting the symbols. For instance, a railroad map and a map
for electric circuits might look very much alike, but their interpretation
differs a lot. In the same sense, a graphical model is a map. Firstly, a
graphical model consists of a graph with dots, lines and potentially edge
marks like arrowheads or circles. Secondly, a graphical model always comes
with a rule for interpreting this graph. In general, nodes in the graph
represent (random) variables and edges represent some kind of dependence.

\subsubsection{Without hidden and selection variables}
An example of a graphical model is the Directed Acyclic Graph (DAG)
model. The physical map here is a graph consisting of nodes and arrows
(only one arrowhead per line) connecting the nodes. As a further
restriction, the arrows must be directed in a way, so that it is not
possible to trace a circle when following the arrowheads. The
interpretation rule for this graph is called d-separation. This rule is a
bit more intricate and we refer the reader to \cite{lauritzen} for more
details. The interpretation rule for this graph can be used in the
following way when given a DAG model: Two nodes $X$ and $Y$ are d-separated
by a set of nodes $S$, if and only if the corresponding random variables
$X$ and $Y$ are conditionally independent given the set of random variables
$S$. For the following, we will only deal with distributions where the
following holds: For each distribution, it is possible to find a DAG, whose
list of d-separation relations perfectly matches the list of conditional
independencies of the distribution. Such distributions are called
faithful. It has been shown that the set of distributions that are faithful
is the overwhelming majority (C. Meek. Strong completeness and faithfulness
in Bayesian networks. In Uncertainty in Artificial Intelligence, pages
411–418, 1995a), so that the assumption does not seem to be very strict in
practice.

Since the DAG model encodes conditional independences, it seems plausible
that information on the latter helps to infer aspects of the former. This
intuition is made precise in the PC algorithm (see \cite{SpirtesEtAl00}; PC stands for the
initials of its inventor Peter Spirtes and Clark Glymour) which was proven
to reconstruct the structure of the underlying DAG model given a
conditional independence oracle up to some ambiguity (equivalence class)
to be discussed below. In practice, the conditional independence oracle is
replaced by a statistical test for conditional independence. For situations
without hidden variables and under some further conditions it has been
shown that the PC algorithm using statistical tests instead of an
independence oracle is computationally feasible and consistent even for very
high-dimensional, sparse DAGs (see \cite{KaBu07a}).

It is possible, that several DAGs encode the same list of conditional
independencies. One can show that such DAGs must share certain
properties. To be more precise, we have to define a v-structure as the
subgraph $i \rightarrow j \leftarrow k$ on the nodes $i$, $j$ and $k$ where
$i$ and $k$ are not connected. Furthermore, let the skeleton of a DAG be
the graph that is obtained by removing all arrowheads from the DAG. It was
shown that two DAGs encode for the same conditional independence statements
if and only if the corresponding DAGs have the same skeleton and the same
 v-structures (see \cite{VermaPearl90}). Two such DAGs are called
equivalent. In this way, the space of DAGs can be partitioned into
equivalence classes, where each member of an equivalence class encodes the
same conditional independence information. Conversely, if given a
conditional independence oracle, one can only determine a DAG up to its
equivalence class. Therefore, the PC algorithm can not determine the DAG
directly, but only the corresponding equivalence class of the DAG. Each DAG
in this equivalence class would be equally valid to describe the
conditional independence information. The equivalence class can be
visualized by a graph that has the same skeleton as every DAG in the
equivalence class and directed edges only where all DAGs in the equivalence
class have the same directed edge. Arrows that point into one direction for
some DAGs in the equivalence class and in the other direction for other
DAGs in the equivalence class are visualized by bidirected edges
(sometimes, undirected edges are used instead). This graph is called
Completed Partially Directed Acyclic Graph (CPDAG).

\begin{algorithm}[h]
\caption{Outline of the PC-algorithm}
\label{pc}
\begin{algorithmic}
\STATE \textbf{INPUT:} Vertex set V, conditional independence information,
significance level $\alpha$\\
\STATE \textbf{OUTPUT:} Estimated CPDAG $\hat{G}$, separation sets $\hat{S}$\\
\STATE \textbf{EDGE TYPES:} $\rightarrow$, $-$\\ 
\STATE \textbf{(P1)} Form the complete undirected graph on the vertex
set V\\
\STATE \textbf{(P2)} Test conditional independence given subsets of
adjacency sets at a given significance level $\alpha$ and delete edges if
conditional independent\\ 
\STATE \textbf{(P3)} Orient v-structures\\
\STATE \textbf{(P4)} Orient remaining edges.\\
\end{algorithmic}
\label{algo:pc}
\end{algorithm}

We will now describe the PC-algorithm, which is shown in Algorithm
\ref{algo:pc}, in more detail. The PC-algorithm starts with a complete
undirected graph, $G_0$, as stated in \textbf{(P1)} of Algorithm
\ref{algo:pc}. In stage \textbf{(P2)}, a series of conditional independence
tests is done and edges are deleted in the following way. First, all pairs
of nodes are tested for marginal independence. If two nodes $i,j$ are
judged to be marginally independent, the edge between them is deleted and
the empty set is saved as separation sets $\hat{S}[i,j]$ and
$\hat{S}[j,i]$. After all pairs have been tested for marginal independence
and some edges might have been removed, a graph results which we denote by
$G_1$. In the second step, all pairs of nodes $i,j$ still adjacent in $G_1$
are tested for conditional independence given any single node in
adj$(G_1,i)\setminus \{j\}$ or adj$(G_1,j)\setminus \{i\}$ (adj$(G,i)$
denotes the set of nodes in graph $G$ that are directly connected to node
$i$ by some edge) . If there is any node $k$ that makes $i,j$ conditionally
independent, the edge between $i$ and $j$ is removed and node $k$ is saved
as separation sets $\hat{S}[i,j]$ and $\hat{S}[j,i]$. If all adjacent pairs
have been tested given one neighboring node, a new graph results which we
denote by $G_2$. The algorithm continues in this way by increasing the size
of the conditioning set step by step. The algorithm stops if all
neighborhoods in the current graph are smaller than the size of the
conditioning set. The result is the skeleton in which every edge is still
undirected. Within \textbf{(P3)}, each triple of vertices $i,k,j$ such that
the pair $i,k$ and the pair $j,k$ are each adjacent in the skeleton but
$i,j$ are not, is oriented based on the information saved in the
conditioning sets $\hat{S}[i,j]$ and $\hat{S}[j,i]$. More precisely,
$i-k-j$ is directed $i \rightarrow k \leftarrow j$ if $k$ is neither in
$\hat{S}[j,i]$ nor in $\hat{S}[i,j]$. Finally, in \textbf{(P4)} it may be
possible to direct some of these edges, since one can deduce that one of
the two possible directions of the edge is invalid because it introduces a
new v-structure or a directed cycle. Such edges are found by repeatedly
applying rules described in \cite{SpirtesEtAl00}, p.85. The resulting output is the
equivalence class (CPDAG) for the given initial graph, in which every edge
is either undirected (or bidirected, which has the same meaning in the
context of the CPDAG) or directed.

\subsubsection{With hidden or selection variables}
In the case of hidden or selection variables, one could still visualize the
underlying causal structure with a DAG that includes all observed, hidden
and selection variables. However, when inferring the DAG from observational
data, we don't know all hidden and selection variables. Therefore, it is
practically unfeasible to find the true underlying DAG. 

A more modest goal could be trying to find a structure, that represents all
independence relationships among the observed variables (but not the latent
and selection variables) of the underlying causal structure. It turns out,
that this is possible. However, the resulting object will in general not be
a DAG for the following reason. Suppose, we have a DAG including
observational, latent and selection variables and we'd like to visualize
the conditional independencies among the observed variables only. We could
marginalize out all latent variables and condition on all selection
variables. It turns out, that the resulting list of conditional
independencies can in general not be represented by a DAG. I.e., DAGs are
not closed under marginalization or conditioning and are therefore not
suited for representing the conditional independence structure on the
observed variables only.

A class of graphical independence models that is closed under
marginalization and conditioning but that contains all DAG independence
models is the class of ancestral graphs. A detailed discussion of this
class of graphs can be found in \cite{RichardsonSpirtes02}. In this text,
we only give a brief introduction to ancestral graphs.

Ancestral graphs have nodes, which represent random variables and edges
which represent some kind of dependence. The edges can be either directed,
undirected or bidirected (note that in the context of ancestral graphs,
undirected and bidirected edges do \emph{not} mean the same). There are two
rules that restrict the direction of edges:
\begin{description}
  \item[1:] If $i$ and $j$ are joined by an edge with an arrowhead at $i$, then
    there is no directed path from $i$ to $j$.
  \item[2:] There are no arrowheads present at a vertex which is an
    endpoint of an undirected edge.
\end{description}
In practice, a slight variation of ancestral graphs is used, that
incorporates a third rule:
\begin{description}
\item[3:] Every missing edge corresponds to a conditional independence.
\end{description}
Ancestral graphs that obey all of the three above rules are called
\emph{Maximal Ancestral Graphs (MAG)}. 

The conditional independence statements of MAGs can be read off using the concept
of m-separation, which is almost identical to the concept of d-separation
in DAGs.

Recall that finding a unique DAG from an independence oracle is in general
impossible. Therefore, one only reports on the equivalence class of DAGs in
which the true DAG must lie. The equivalence class is visualized using a
CPDAG. The same is true for MAGs: Finding a unique MAG from an independence
oracle is in general impossible. One only reports on the equivalence class
in which the true MAG lies. 

An equivalence class of a MAG can be uniquely represented by a
\emph{Partial Ancestral Graph (PAG)}. A PAG contains the following
types of edges: o-o, o-, o->, ->, <->, -. Roughly, the bidirected edges come
from hidden variables, and the undirected edges come from selection
variables. The edges have the following interpretation: (i) there is
an edge between $x$ and $y$ if and only if $x$ and $y$ are
conditionally dependent given S for all sets S consisting of all
selection variables and a subset of the observed variables; (ii) a
tail on an edge means that this tail is present in all MAGs in the
equivalence class; (iii) an arrowhead on an edge means that this
arrowhead is present in all MAGs in the equivalence class; (iv) a
o-edgemark means that there is a at least one MAG in the equivalence
class where the edgemark is a tail, and at least one where the
edgemark is an arrowhead.

An algorithm for finding the PAG given an independence oracle is the FCI
algorithm (see \cite{SpirtesEtAl00}). This algorithm was slightly extended and proven to be
sound and complete in \cite{Zhang08-orientation-rules}. We refer the reader
to \cite{Zhang08-orientation-rules} for a detailed discussion of the FCI algorithm. Suffice it to say,
that FCI is very similar to PC but makes additional conditional
independence tests and uses more orientation rules (see section
\ref{sec:fci} for more details).

\subsection{Estimating bounds on causal effects}
One way of quantifying the causal effect of variable $X$ on $Y$ is to
measure the state of $Y$ if $X$ is forced to take value $X=x$. If $X$ and
$Y$ are random variables, forcing $X=x$ could have the effect of changing
the distribution of $Y$. Following the conventions in \cite{Pearl00}, the resulting
distribution after manipulation is denoted by $P[Y | do(X=x)]$. Note that
this is different from the conditional distribution $P[Y | X=x]$. To
illustrate this, imagine the following simplistic situation. Suppose we
observe every hour a particular spot on the street. The random variable $X$
denotes whether it rained during that hour ($X=1$ if it rained, $X=0$
otherwise). The random variable $Y$ denotes whether the street was wet or
damp at the end of that hour we looked ($Y=1$ if it was wet, $Y=0$
otherwise). If we assume $P(X=1) = 0.1$ (rather dry region), $P(Y=1|X=1) =
0.99$ (the street is almost always still wet when we look after and it
rained during the last hour) and $P(Y=1|X=0) = 0.02$ (other reasons for
making the street wet are rare), we can compute the conditional probability
$P(X=1|Y=1) = 0.85$. So, if we observe the street to be wet, the
probability that there was rain in the last hour is about $0.85$. However,
if we take a garden hose and force the street to be wet at a randomly
chosen hour, we get $P(X=1|do(Y=1)) = P(X=1) = 0.1$. Thus, the distribution
of the random variable describing rain is quite different when making an
observation and making an intervention.  Oftentimes, only the change of the
target distribution under intervention is reported. We will use the change
in mean, i.e. $\frac{\partial}{\partial x} E[Y|do(X=x)]$, as measure for
the causal effect for Gaussian random variables. For Gaussian random
variable $Y$, $E[Y|do(X=x)]$ depends linearly on $x$. Therefore, the
derivative is constant which means that the causal effect does not depend
on $x$. For binary random variables (with domain $\{0,1\}$) we use
$E(X|do(Y=1)) - E(X|do(Y=0)) = P(X=1|do(Y=1)) - P(X=1|do(Y=0))$.

The goal in the remaining section is to estimate the effect of an
intervention if only observational data is available. 
\subsubsection{Without hidden and selection variables}
If the causal structure is a known DAG and there are no hidden and
selection variables, \cite{Pearl00} (Th 3.4.1) suggested a set of inference rules
known as ``do-calculus'' whose
application transforms an expression involving a ``do'' into an expression
involving only conditional distributions. Thus, information on the
interventional distribution can be obtained by using information obtained
by observations and knowledge of the underlying causal structure. \cite{shpitser}
restructured the rules of ``do-calculus'' into algorithmic form and showed
that they are complete in a sense that, if the algorithm doesn't find a
transformation, there exists none.

Unfortunately, the causal structure is rarely known in practice. Under some
assumptions, \cite{Pearl00} showed (Th. 1.4.1) that there is a link between
causal structures and graphical models. Roughly speaking, if the underlying
causal structure is a DAG, we observe data generated from this DAG and then
estimate a DAG model (i.e. a graphical model) on this data, the estimated
CPDAG represents the equivalence class of the DAG model describing the
causal structure. This holds if we have enough samples and assuming that
the true underlying causal structure is indeed a DAG without latent or
selection variables. Note that even given an infinite amount of data, we
usually cannot identify the true DAG itself, but only its equivalence
class. Every DAG in this equivalence is equally likely to represent the
true causal structure. Taking this into account, we conceptually apply the
do-calculus on each DAG within the equivalence class and thus obtain not a
unique value for a causal effect but one value for each DAG in the causal
structure (in practice, we developed a local method that is faster but
yields a similar result; see \ref{sec:ida} for more details). Therefore,
even if we have an infinite amount of observations we can only report on a
multiset of possible causal values (it is a multiset rather than a set
because it can contain duplicate values). One of these values is the true
causal effect. Despite the inherent ambiguity, this result can oftentimes
still be very useful, when the set has certain properties (e.g. all values
are much larger than zero).

In addition to this fundamental limitation in estimating a causal effect,
errors due to finite sample size blur the result as with every statistical
method. Thus, in a real world situation, we only get an estimate of the set
of possible causal values. It was shown under some assumptions, that this
estimate is consistent (see \cite{BuKaMa10}).

It has recently been shown empirically that despite the described
fundamental limitations in identifying the causal effect uniquely, the
method performs well in identifying the most important causal effects in a
high-dimensional setting (see \cite{NatMethods10}).

\section{Package pcalg}
This package has three goals. First, it is intended to provide fast,
flexible and reliable implementations of the PC and the FCI
algorithm. Secondly, it is intended to provide a tool for estimating the
causal effect among continuous variables given a causal structure
using a for of the do-calculus known as back-door criterium. Finally, combining
these two applications of the package, it is possible to estimate causal
effects when no causal structure is known. In the following, we describe
the main functions of our package for achieving these goals. The functions
\code{skeleton}, \code{pc} and \code{fci} are intended for estimating
graphical models. The functions \code{ida} and \code{idaFast} are intended
for estimating causal effects when the causal structure is known or was
estimated.

\subsection{skeleton}\label{sec:skel}
The function \code{skeleton} implements (P1) and (P2) from Algorithm
\ref{algo:pc}.The function can be called with the following arguments.

\code{skeleton(suffStat, indepTest, p, alpha, verbose = FALSE, fixedGaps = NULL,\\ fixedEdges = NULL, NAdelete = TRUE, m.max = Inf)}

As was shown in section \ref{sec:gm}, the main task in finding the
skeleton is to compute and test several conditional independencies. To
keep the function flexible, \code{skeleton} takes as argument a
function \code{indepTest} that performs these independence tests and
returns a p-value. All information that is needed in the independence
test can be passed in the argument \code{suffStat}. The only
exceptions are the number of variables \code{p} and the significance
level $\alpha$ for the conditional independence tests, which are
passed separately. For convenience, we have preprogrammed versions of
\code{indepTest} for Gaussian data (\code{gaussCItest}), discrete data
(\code{disCItest}), and binary data (see \code{binCItest}). Each of
these independence test functions need different arguments as input,
which are described in the helpfiles of the functions. For example,
when using \code{gaussCItest}, the input has to be a list containing
the correlation matrix and the sample size of the data.  In the
following code, we estimate the skeleton on the data set
\code{gaussianData} (which consists of $p=8$ variables and $n=5000$
samples) and plot the results. The estimated skeleton and the true
underlying DAG are shown in Fig. \ref{fig:skelExpl}.

<<skelExpl1>>=
require(pcalg)
data(gaussianData)
indepTest <- gaussCItest 
suffStat <- list(C = cor(datG), n = 5000)
pc.fit <- skeleton(suffStat, indepTest, p = 8, alpha = 0.01)
par(mfrow = c(1,2))
plot(gGtrue, main = "True DAG")
plot(pc.fit, main = "Estimated Skeleton")
@ 
\begin{figure}
  \begin{center}
<<skelExpl1Plot, fig = TRUE, echo = FALSE>>=
par(mfrow = c(1,2))
plot(gGtrue, main = "True DAG")
plot(pc.fit, main = "Estimated Skeleton")
@ 
\caption{True DAG and estimated skeleton fitted on the Gaussian data set
  provided by the package using the function \code{skeleton}.}
\label{fig:skelExpl}
\end{center}
\end{figure}

To give another example, we show how to fit a skeleton to the example
data set \code{discreteData} (which consists of $p=5$ discrete
variables with 3, 2, 3, 4 and 2 levels and $n=10000$ samples). The
predefined test function \code{disCItest} is based on the $G^2$
statistics and takes as input a list containing the data matrix, a
vector specifying the number of levels for each variable and an option
which indicates whether to lower the degrees of freedom by one for
each zero count. Finally, we plot the result. The estimated skeleton
and the true underlying DAG are shown in Fig. \ref{fig:skel2}.

<<skelExp2>>=
data(discreteData)
p <- ncol(datD)
indepTest <- disCItest
suffStat <- list(dm = datD, nlev = c(3,2,3,4,2), adaptDF = FALSE)
alpha <- 0.01 
skel.fit <- skeleton(suffStat, indepTest, p, alpha)
par(mfrow = c(1,2))
plot(gDtrue, main = "True DAG")
plot(skel.fit, main = "Estimated Skeleton")
@ 
\begin{figure}
  \begin{center}
<<skelExp2Plot, fig = TRUE, echo = FALSE>>=
par(mfrow = c(1,2))
plot(gDtrue, main = "True DAG")
plot(skel.fit, main = "Estimated Skeleton")
@ 
\caption{True DAG and estimated skeleton fitted on the discrete data set provided by the package using the function \code{skeleton}.}
\label{fig:skel2}
\end{center}
\end{figure}

The information in \code{fixedGaps} and \code{fixedEdges} is used as
follows.  The gaps given in \code{fixedGaps} are introduced in the very
beginning of the algorithm by removing the corresponding edges from the
complete undirected graph. Thus, these gaps are guaranteed to be in the
resulting graph. Pairs $(i,j)$ in \code{fixedEdges} are skipped in
all steps of the algorithm, so that these edges are guaranteed to be in the
resulting graph.

If \code{indepTest} returns \code{NA} and the option \code{NAdelete} is
\code{TRUE}, the corresponding edge is deleted. If this option is
\code{FALSE}, the edge is not deleted.

The argument \code{m.max} is the maximal size of the conditioning sets that
are considered in the conditional independence tests in (P2) of Algorithm \ref{algo:pc}.
          
Throughout, the function works with the column positions of the
variables in the adjacency matrix, and not with the names of the variables.

\subsection{pc} \label{sec:pc} 

The function \code{pc} implements all steps (P1) to (P4) of the PC
algorithm. First, the skeleton is computed using the function
\code{skeleton} (steps (P1) and (P2)). Then, as many edges as possible are
directed (steps (P3) and (P4)). The function can be called with the
following arguments.

\code{pc(suffStat, indepTest, p, alpha, verbose = FALSE, fixedGaps = NULL, \\
fixedEdges = NULL, NAdelete = TRUE, m.max = Inf, u2pd = "rand", \\
conservative = FALSE)}
     
The options \code{suffStat, indepTest, p, alpha, fixedGaps,
  fixedEdges, NAdelete} and \code{m.max} are identical to those of the
function \code{skeleton}.

Sampling errors (or hidden variables) can lead to conflicting information
about edge directions. For example, one may find that $a-b-c$ and $b-c-d$
should both be directed as v-structures.  This gives conflicting
information about the edge $b-c$, since it should be directed as $b
\leftarrow c$ in v-structure $a \rightarrow b \leftarrow c$, while it
should be directed as $b \rightarrow c$ in v-structure $b \rightarrow c
\leftarrow d$. In such cases, we simply overwrite the directions of the
conflicting edge. In the example above this means that we obtain $a
\rightarrow b \rightarrow c \leftarrow d$ if $a-b-c$ was visited first, and
$a \rightarrow b \leftarrow c \leftarrow d$ if $b-c-d$ was visited first.

Sampling errors or hidden variables can also lead to invalid CPDAGs,
meaning that there does not exist a DAG that has the same skeleton and
v-structures as the graph found by the algorithm. An example of this is an
undirected cycle consisting of the edges $a-b-c-d$ and $d-a$. In this case it
is impossible to direct the edges without creating a cycle or a new
v-structure. The option \code{u2pd} specifies what should be done in such a
situation. If the option is set to \code{"relaxed"}, the algorithm simply
outputs the invalid CPDAG. If the option is set to \code{"rand"}, all direction
information is discarded and a random DAG is generated on the skeleton. If
the option is set to \code{"retry"}, up to 100 combinations of possible
directions of the ambiguous edges are tried, and the first combination that
results in a valid CPDAG is chosen. If no valid combination is found, an
arbitrary DAG is generated on the skeleton as in the option \code{"rand"}.

Using the option \code{conservative = TRUE}, the conservative PC
algorithm done. The conservative PC is a slight variation of the PC
algorithm and is intended to be more robust against sampling errors. After
the skeleton is computed, all potential v-structures $a-b-c$ are checked in
the following way. We test whether $a$ and $c$ are independent conditioning
on any subset of the neighbors of a or any subset of the neighbors of
$c$. If $b$ is in no such conditioning set (and not in the original sepset)
or in all such conditioning sets (and in the original sepset), no further
action is taken and the usual PC is continued. If, however, $b$ is in only
some conditioning sets, the triple $a-b-c$ is marked
'unfaithful'. Moreover, if in the conservative step there is no subset
among the neighbors that makes $a$ and $c$ independent, the triple is
marked 'unfaithful'. An unfaithful triple is not oriented as a v-structure.
Furthermore, no later orientation rule that needs to know whether $a-b-c$
is a v-structure or not is applied. (For more detail, see the help file of
the internal function \code{pc.cons.intern} which is called with the argument
\code{version.unf = c(2,2)}.)  

As with the skeleton, the algorithm works with the column positions of the
variables in the adjacency matrix, and not with the names of the
variables. When plotting the object, undirected and bidirected edges are
equivalent.

As an example, we estimate a CPDAG of the Gaussian data used in the
example for the skeleton in section \ref{sec:skel}. Again, we choose
the predefined function \code{gaussCItest} as conditional independence
test and create the corresponding test statistic. Finally, we plot the
result. The estimated CPDAG and the true underlying DAG are shown in
Fig. \ref{fig:pcFit1}.

<<pcExpl>>=
data(gaussianData)
indepTest <- gaussCItest 
suffStat <- list(C = cor(datG), n = 5000)
pc.fit <- pc(suffStat, indepTest, p = 8, alpha = 0.01)
par(mfrow = c(1,2))
plot(gGtrue, main = "True DAG")
plot(pc.fit, main = "Estimated CPDAG")
@ 
\begin{figure}
  \begin{center}
<<fig = TRUE, echo = FALSE>>=
## plot the resulting CPDAG
par(mfrow = c(1,2))
plot(gGtrue, main = "True DAG")
plot(pc.fit, main = "Estimated CPDAG")
@ 
\caption{True DAG and estimated CPDAG fitted on the Gaussian data set provided by the
  package using the function \code{pc}.}
\label{fig:pcFit1}
\end{center}
\end{figure}

\subsection{fci}\label{sec:fci}

This function is a generalization of the PC algorithm (see section
\ref{sec:pc}), in the sense that it allows arbitrarily many latent and
selection variables. Under the assumption that the data are faithful to a
DAG that includes all latent and selection variables, the FCI algorithm
(Fast Causal Inference algorithm) estimates the equivalence class of MAGs
that describe the conditional independence relationships between the
observed variables.

The first part of the FCI algorithm is analogous to the PC algorithm. It
starts with a complete undirected graph and estimates an initial skeleton
using the function \code{skeleton}. All edges of this skeleton are of the
form o-o. Now, the v-structures are directed as in the PC algorithm. Due to the presence of hidden variables, it is no longer
sufficient to consider only subsets of the neighborhoods of nodes $x$ and
$y$ to decide whether the edge $x-y$ should be removed.  Therefore, the
initial skeleton may contain some superfluous edges.  These edges are
removed in the next step of the algorithm. To decide whether edge $x$ o-o $y$
should be removed, one computes Possible-D-SEP($x$) and Possible-D-SEP($y$)
and performs conditional independence tests of $x$ and $y$ given all
possible subsets of Possible-D-SEP($x$) and of Possible-D-SEP($y$) (see
helpfile of function \code{pdsep}). Subsequently, all edges are transformed into o-o again and the v-structures are
determined (using information in sepset).  Finally, as many as possible
undetermined edge marks (o) are determined using (a subset of) the 10
orientation rules given by \cite{Zhang08-orientation-rules}.

The function can be called with the following arguments:

\code{fci(suffStat, indepTest, p, alpha, verbose = FALSE, fixedGaps = NULL,\\ 
  fixedEdges = NULL, NAdelete = TRUE, m.max = Inf, rules = rep(TRUE, 10),\\ 
  doPdsep = TRUE, conservative = c(FALSE, FALSE), biCC = FALSE, \\
  cons.rules = FALSE))}
     
The options \code{suffStat, indepTest, p, alpha, fixedGaps, fixedEdges,
  NAdelete} and \code{m.max} are identical to those in function
\code{skeleton}. The option \code{rules} contains a logical vector of
length 10 indicating which rules should be used when directing edges. The
order of the rules is taken from \cite{Zhang08-orientation-rules}.
     
The option \code{doPdsep} indicates, whether Possible-D-SEP should be
computed. If TRUE, Possible-D-SEP is computed for all nodes, and all
subsets of Possible-D-SEP are considered as conditioning sets in the
conditional independence tests. If FALSE, Possible-D-SEP is not computed,
so that the algorithm simplifies to the Modified PC algorithm of \cite{SpirtesEtAl00}.

Using the option \code{conservative} one can invoke the conservative
FCI. The conservative FCI consists of two parts. In the first part (done if
the first argument of \code{conservative} is \code{TRUE}), we call the
internal function \code{pc.cons.internal} with option \code{version.unf
  = c(1,2)} after computing the skeleton. This is a slight variation of the
conservative PC (which used \code{version.unf = c(2,2)}): If $a$ is
independent of $c$ given some $S$ in the skeleton (i.e., the edge $a-c$
dropped out), but $a$ and $c$ remain dependent given all subsets of
neighbors of either $a$ or $c$, we will call all triples $a-b-c$
'faithful'. This is because in the FCI, the true separating set might be
outside the neighborhood of either $a$ or $c$. In the second part (done if
the second argument of \code{conservative} is \code{TRUE}), we call
\code{pc.cons.internal} with option \code{version.unf = c(1,2)} again after
Possible-D-Sep was found and the graph potentially lost some
edges. Therefore, new triples might have occurred. If this second part is
done, the resulting information on sepset and faithful triples overwrites
the previous and will be used for the subsequent orientation rules.

Using the option \code{biCC = TRUE} only nodes on paths between $a$ and $c$
are considered to be in \code{sepset(a,c)}. This method uses biconnected
components to find all paths between nodes $a$ and $c$.

The argument \code{cons.rules} manages the way in which the information
about unfaithful triplets affects the orientation rules for directing
edges. If \code{cons.rules = TRUE}, an orientation rule that needs
information on definite non-colliders is only applied, if the corresponding
subgraph relevant for the rule does not involve an unfaithful triple.

As an example, we estimate the PAG of a graph with five nodes using
the function \code{fci} and the predefined function \code{gaussCItest}
as conditional independence test. Finally, we plot the result. Node $V1$ is latent. In
Fig. \ref{fig:fci} the true DAG and the estimated PAG are shown.
          
<<fciExpl>>=
data(latentVarGraph)
dim(datL)
suffStat1 <- list(C = cor(datL), n = nrow(datL))
indepTest1 <- gaussCItest 
pag.est <- fci(suffStat1, indepTest1, ncol(datL), alpha = 0.01, verbose=FALSE)
par(mfrow = c(1,2))
plot(g1, main = "True DAG")
plot(pag.est, main = "Estimated PAG")
@

\begin{figure}
  \begin{center}
<<fig = TRUE, echo = FALSE>>=
par(mfrow = c(1,2))
plot(g1, main = "True DAG")
plot(pag.est, main = "Estimated PAG")
@ 
\caption{True data generating DAG and estimated PAG. Variable 1 of the data
generating DAG is latent. One can see that the PAG was estimated correctly.}
\label{fig:fci}
\end{center}
\end{figure}

\subsection{ida} \label{sec:ida}
It is assumed that we have observational data that are multivariate
Gaussian and faithful to the true (but unknown) underlying causal DAG
(without hidden variables).  Under these assumptions, this function
estimates the multiset of possible total causal effects of $x$ on $y$,
where the total causal effect is defined via Pearl's do-calculus as
$\frac{\partial}{\partial z} E[Y|do(X=z)]$ (this value does not depend on $z$, since
Gaussianity implies that conditional expectations are linear). We estimate
a multiset of possible total causal effects instead of the unique total causal
effect, since it is typically impossible to identify the latter when the
true underlying causal DAG is unknown (even with an infinite amount of
data).

To illustrate this, consider the following example. We have data from seven
Gaussian variables with a causal structure given on the left of
Fig. \ref{fig:ida}. Denote node $i$ in the graph by $Vi$. We assume that
the causal structure is unknown and want to infer the causal effect of $V2$
on $V5$. First, we estimate the equivalence class of DAGs that describe the
conditional independence relationships in the data, using the function
\code{pc} (see section \ref{sec:pc}).

<<idaExpl1>>=
data(idaData)
dim(datI)
indepTest <- gaussCItest 
suffStat <- list(C = cor(datI), n = nrow(datI))
pc.fit <- pc(suffStat, indepTest, p = ncol(datI), alpha = 0.01)
@ 

Comparing the true DAG with the CPDAG in Fig. \ref{fig:ida}, we see that
the CPDAG and the true DAG have the same skeleton. Moreover, the directed
edges in the CPDAG are also directed in that way in the true DAG. Three
edges in the CPDAG are bidirected. Recall that undirected and bidirected
edges bear the same meaning in a CPDAG, so they can be used interchangeably.
\begin{figure}
  \begin{center}
<<idaExpl2, fig = TRUE, echo = FALSE>>=
par(mfrow = c(1,2))
plot(myDAG, main = "True DAG")
plot(pc.fit, main = "Estimated CPDAG")
@ 
\caption{True DAG and estimated CPDAG for illustrating the function \code{ida}}
\label{fig:ida}
\end{center}
\end{figure}

Since there are three undirected edges in the CPDAG, there might be up to
$2^3 = 8$ DAGs in the corresponding equivalence class. However, the
undirected edges $V2-V3-V5$ can be formed to a new v-structures. As
mentioned in section \ref{sec:gm}, DAGs in the equivalence class must have
exactly the same v-structures as the corresponding CPDAG. Thus, $V2-V3-V5$
can only be directed as $V2 \rightarrow V3 \rightarrow V5$, $V2 \leftarrow
V3 \leftarrow V5$ or $V2 \leftarrow V3 \rightarrow V5$ but not as $V2
\rightarrow V3 \leftarrow V5$, since this would introduce a new colliding
v-structure. There is only one remaining undirected edge ($V1-V6$), which
can be directed in two ways. Thus, there are six valid DAGs 
(i.e. they have no new v-structure and no cycle) and these form the
equivalence class represented by the CPDAG. In Fig. \ref{fig:allDags}, all
DAGs in the equivalence class are shown. DAG 4 is the true DAG.

<<idaExpl3, echo = FALSE>>=
am.pdag <- wgtMatrix(pc.fit@graph)
ad <- allDags(am.pdag, am.pdag, NULL)
gDag <- vector("list", nrow(ad))
for (i in 1:nrow(ad)) gDag[[i]] <- as(matrix(ad[i, ], 7, 7), "graphNEL")
par(mfrow = c(3,2))
for (i in 1:6) plot(gDag[[i]], main = paste("DAG",i))
@ 
\begin{figure}
  \begin{center}
<<fig = TRUE, echo = FALSE>>=
par(mfrow = c(1,2))
for (i in 1:2) plot(gDag[[i]], main = paste("DAG",i))
@ 
<<fig = TRUE, echo = FALSE>>=
par(mfrow = c(1,2))
for (i in 3:4) plot(gDag[[i]], main = paste("DAG",i))
@ 
<<fig = TRUE, echo = FALSE>>=
par(mfrow = c(1,2))
for (i in 5:6) plot(gDag[[i]], main = paste("DAG",i))
@ 
\caption{All DAGs belonging to the same equivalence class in which the true
  DAG shown in Fig. \ref{fig:ida} is.}
\label{fig:allDags}
\end{center}
\end{figure}

For each DAG G in the equivalence class, we apply Pearl's do-calculus to
estimate the total causal effect of $x$ on $y$. This can be done via a
simple linear regression: if $y$ is not a parent of $x$, we take the
regression coefficient of $x$ in the regression \code{lm(y ~ x +
  pa(x))}, where \code{pa(x)} denotes the parents of $x$ in the DAG G; if
$y$ is a parent of $x$ in G, we set the estimated causal effect to zero.

If the equivalence class contains $k$ DAGs, this will yield $k$ estimated
total causal effects. Since we do not know which DAG is the true causal
DAG, we do not know which estimated total causal effect of $x$ on $y$ is
the correct one. Therefore, we return the entire multiset of $k$ estimated
effects.

In our example, there are six DAGs in the equivalence class. Therefore, the
function \code{ida} (using the option \code{method = global}) will
produce six possible values of causal effects (one for each DAG).

<<idaExpl4>>=
     ida(2, 5, cov(datI), pc.fit@graph, method = "global", verbose = FALSE)
@ 

Among these six values, there are only two unique values: $-0.0049$ and
$0.5421$. This is because we compute \code{lm(V5 ~ V2 + pa(V2))} ($Vi$
denotes node $i$) for each
DAG and report the regression coefficient of $V2$. Note that there are
only two possible parent sets of $V2$ in all six DAGs: In DAGs 3 and 6,
there are no parents of $V2$. In DAGs 1, 2, 4 and 5, however, the parent
of $V2$ is $V3$. Thus, exactly the same regressions are computed for
DAGs 3 and 6 and the same regressions are computed for DAGs 1, 2, 4 and
5. Therefore, we end up with two unique values, one of which occurs two
times, while the other occurs four times.

Since the data was simulated from a model, we know that the true value of the
causal effect of $V2$ on $V5$ is $0.5529$. Thus, one of the two unique values is
indeed very close to the true causal effect (the slight discrepancy is due
to sampling error). 

The function \code{ida} can be called with the following arguments.

\code{ida(x.pos, y.pos, mcov, graphEst, method = "local", y.notparent = FALSE,\\ 
verbose = FALSE, all.dags = NA)},

where \code{x.pos} denotes the column position of the cause variable, 
\code{y.pos} denotes the column position of the effect
variable, \code{mcov} is the covariance matrix of the original
data, and \code{graphEst} is a graph object describing the causal structure
(this could be given by experts or estimated using the function \code{pc}).

If \code{method="global"}, the method is carried out as described above,
where all DAGs in the equivalence class of the estimated CPDAG are
computed. This method is suitable for small graphs (say, up to 10
nodes). The DAGs can (but need not) be precomputed using the function
\code{allDags} and passed via the option \code{all.dags}.

If \code{method="local"}, we do not determine all DAGs in the equivalence
class of the CPDAG. Instead, we only consider the local neighborhood of $x$
in the CPDAG. In particular, we consider all possible directions of
undirected edges that have $x$ as endpoint, such that no new v-structure is
created. For each such configuration, we estimate the total causal effect
of $x$ on $y$ as above, using linear regression.

At first sight, it is not clear that such a local configuration corresponds
to a DAG in the equivalence class of the CPDAG, since it may be impossible
to direct the remaining undirected edges without creating a directed cycle
or a v-structure. However, \cite{BuKaMa10} showed
that there is at least one DAG in the equivalence class for each such local
configuration.  As a result, it follows that the multisets of total causal
effects of the \code{global} and the \code{local} method have the same unique
values. They may, however, have different multiplicities. 

Recall, that in the example using the global method, we obtained two unique
values with multiplicities two and four yielding six numbers in total.
Applying the local method, we obtain the same unique values, but the
multiplicities are 1 for both values (they need not always be one).

<<idaExpl5>>=
ida(2,5,cov(datI),pc.fit@graph,method = "local")
@ 

One can take summary measures of the multiset. For example, the minimum
absolute value provides a lower bound on the size of the true causal
effect: if the minimum absolute value of all values in the multiset is
larger than one, then we know that the size of the absolute true causal
effect (up to sampling error) must be larger than one. The fact that the
unique values of the multisets of the \code{global} and \code{local}
method are identical implies that summary measures of the multiset that
only depend on the unique values (such as the minimum absolute value) can
be found using the local method.

In some applications, it is clear that some variable is definitively not an
effect but it might be a cause. Consider for example a bacterium producing
a certain substance, taking the amount of produced substance as response
variable. Knocking out genes in the bacterium might change the ability to
produce the substance. By measuring the expression levels of genes, we want
to know which genes have a causal effect on the product. In this setting,
it is clear that the amount of substance is the effect and the activity of
the genes is the cause. Thus in the causal structure, the response variable
can not be a parent node of any variable describing the expression level of
genes. This background knowledge can be easily incorporated: By setting the
option \code{y.notparent = TRUE}, all edges in the CPDAG connected to the
response variable (no matter whether directed or undirected) are overwritten so
that they are directed towards the response variable at the end.

\subsection{idaFast}
In some applications it is desirable to estimate the causal effect of one
variable on a set of response variables. In these situations, the function
\code{idaFast} should be used. Imagine for example, that we have data
on several variables, that we have no background knowledge about the causal
effects among the variables and that we want to estimate the causal effect of
each variable onto each other variable. To this end, we could consider for
each variable the problem: What is the causal effect of this variable on
all other variables. Of course, one could solve the problem by using \code{ida} on
each pair of variables. However, there is a more efficient way which uses
the fact, that a linear regression of a fixed set of explanatory variables
on several different response variables can be computed very efficiently.

The function \code{idaFast} can be called with the following arguments:
\code{idaFast(x.pos, y.pos.set, mcov, graphEst)}. The arguments
\code{x.pos}, \code{mcov}, \code{graphEst} have the same meaning as
the corresponding arguments in \code{ida}. The argument \code{y.pos.set}
is a vector containing the column positions of all response variables of
interest.

This call performs \code{ida(x.pos, y.pos, mcov, graphEst, method="local",
  y.notparent=FALSE, verbose=FALSE)} for all values of \code{y.pos} in
\code{y.pos.set} at the same time and in an efficient way. Note that the
option \code{y.notparent = TRUE} is not implemented.
%%- , since it is not
%%- clear how to do that efficiently without orienting all edges away from
%%- \code{y.pos.set} at the same time, which seems not to be desirable.

Consider the example from section \ref{sec:ida}, where we computed the
causal effect of $V2$ on $V5$. Now, we want to compute the effect of
$V2$ on $V5$, $V6$ and $V7$ using \code{idaFast}. In the following
example, we first simulate data and compute the true DAG, CPDAG and
covariance matrix. Then, we estimate the causal effects using
\code{ida} and \code{idaFast}.
<<>>=
set.seed(123)
p <- 7
myDAG <- randomDAG(p, prob = 0.2) ## true DAG
myCPDAG <- dag2cpdag(myDAG) ## true CPDAG
covTrue <- trueCov(myDAG) ## true covariance matrix

n <- 10000
dat <- rmvDAG(n, myDAG)

alpha <- 0.01
indepTest <- gaussCItest 
suffStat <- list(C = cor(dat), n = n)
pc.fit <- pc(suffStat, indepTest, p, alpha)

(eff.est1 <- ida(2,5,cov(dat),pc.fit@graph,method="local",verbose=FALSE))
(eff.est2 <- ida(2,6,cov(dat),pc.fit@graph,method="local",verbose=FALSE))
(eff.est3 <- ida(2,7,cov(dat),pc.fit@graph,method="local",verbose=FALSE))

(eff.estF <- idaFast(2,c(5,6,7),cov(dat),pc.fit@graph))
@ 

\subsection{Using a user specific conditional independence test} 

In some cases it might be desirable to use a user specific conditional
independence test instead of the provided ones. The \pkg{pcalg}
package allows the use of any conditional independence test defined by
the user. In this section, we illustrate this feature by using a
conditional independence test for Gaussian data that is not predefined
in the package.

The functions \code{skeleton}, \code{pc} and \code{fci} need a
  function of the form \code{myCItest(x, y, S, suffStat)} to test
  conditional independence relationships. This function must return
  the p-value of the conditional independence test of $x$ and $y$ given
  $S$ given some information on the data in the form of a sufficient
  statistic (this might be simply the data frame with the original
  data), where $x$, $y$, $S$ indicate column positions of the original data
  matrix. We will show an example that illustrates how to construct such a
  function.

A simple way to compute the partial correlation of $x$ and $y$ given $S$
for some data is to solve the two associated linear regression problems $x
\sim S$ and $y \sim S$, get the residuals, and calculate the correlation
between the residuals. Finally, a correlation test between the residuals
yields a p-value that can be returned. The argument \code{suffStat} is an
arbitrary object containing several pieces of information that are all used
within the function to produce the p-value. In the predefined function
\code{gaussCItest} for example, a list containing the correlation matrix
and the number of observations is passed. This has the advantage, that any
favorite (e.g. robust) method of computing the correlation matrix can be
used before partial correlations are computed. Oftentimes, however, it will
suffice to just pass the complete data set in \code{suffStat}. We will
choose this simple method in our example. An implementation of the function
\code{myCItest} could look like this.
<<>>=
myCItest <- function(x,y,S, suffStat) {
  if (length(S) == 0) {
    res <- cor.test(suffStat[,x],suffStat[,y])$p.value
  } else {
    resx <- resid(lm(suffStat[,x] ~ suffStat[,S]))
    resy <- resid(lm(suffStat[,y] ~ suffStat[,S]))
    res <- cor.test(resx, resy)$p.value
  }
  res
}
@ 
We can now use this function together with the \code{pc} function.
<<echo = FALSE>>=
data(gaussianData)
indepTest <- gaussCItest 
suffStat <- list(C = cor(datG), n = 5000)
pc.fit <- pc(suffStat, indepTest, p = 8, alpha = 0.01)
@ 

<<>>=
data(gaussianData) ## contains data matrix datG
indepTest <- myCItest 
suffStat <- datG
pc.myfit <- pc(suffStat, indepTest, p = 8, alpha = 0.01)
par(mfrow = c(1,2))
plot(pc.fit, main = "gaussCItest")
plot(pc.myfit, main = "myCItest")
@ 

\begin{figure}
  \begin{center}
<<fig = TRUE, echo = FALSE>>=
par(mfrow = c(1,2))
plot(pc.fit, main = "gaussCItest")
plot(pc.myfit, main = "myCItest")
@ 
\caption{The estimated CPDAGs using the predefined and the user specified
  conditional independence test are identical.}
\label{fig:userSpec}
\end{center}
\end{figure}
As expected, the resulting CPDAG (see Fig. \ref{fig:userSpec}) is the same
as in section \ref{sec:pc} where we used the function \code{gaussCItest} as
conditional independence test. Note, however, that using
\code{gaussCItest} is considerably faster than using \code{myCItest} (on
our computer $0.067$ seconds using \code{gaussCItest} versus $4.448$
seconds using \code{myCItest}).

??? MENTION: S4-class gAlgo, pcAlgo, fciAlgo and inheritance.
\bibliography{Mybib}

\end{document}
